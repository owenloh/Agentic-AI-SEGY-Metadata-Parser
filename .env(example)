# LLM Provider Configuration
# Copy this file to .env and configure your settings

# Default LLM provider to use (local or gemini)
DEFAULT_LLM_PROVIDER=local

# Fallback providers (comma-separated)
FALLBACK_LLM_PROVIDERS=gemini

# Local LLM Configuration (OpenAI-compatible API)
LOCAL_LLM_SERVER_URL=http-your-local-llm-url
LOCAL_LLM_MODEL=meta-llama/Meta-Llama-3.3-70B-Instruct
LOCAL_LLM_TEMPERATURE=0.4
LOCAL_LLM_MAX_TOKENS=120000
LOCAL_LLM_TIMEOUT=30
LOCAL_LLM_MAX_RETRIES=3

# Gemini Configuration
GEMINI_API_KEY=your-api-key
GEMINI_MODEL=gemini-1.5-flash
GEMINI_TIMEOUT=30
GEMINI_MAX_RETRIES=1