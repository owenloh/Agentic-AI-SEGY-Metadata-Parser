# LLM Provider Configuration Template
# Copy this file to .env and configure your settings
# NEVER commit .env to version control!

# Default LLM provider to use (gemini or local)
DEFAULT_LLM_PROVIDER=gemini

# Fallback providers (comma-separated)
FALLBACK_LLM_PROVIDERS=gemini

# Gemini Configuration (Recommended - Free)
# Get your free API key from https://aistudio.google.com/
GEMINI_API_KEY=your_gemini_api_key_here
GEMINI_MODEL=gemini-1.5-flash
GEMINI_TIMEOUT=30
GEMINI_MAX_RETRIES=1

# Optional: Local LLM Configuration (OpenAI-compatible API)
# Uncomment and configure if you have access to a local LLM
# DEFAULT_LLM_PROVIDER=local
# LOCAL_LLM_SERVER_URL=http://your-local-llm-server/v1/chat/completions
# LOCAL_LLM_API_KEY=your_local_api_key_if_needed
# LOCAL_LLM_MODEL=your_model_name
# LOCAL_LLM_TEMPERATURE=0.4
# LOCAL_LLM_MAX_TOKENS=120000
# LOCAL_LLM_TIMEOUT=30
# LOCAL_LLM_MAX_RETRIES=3